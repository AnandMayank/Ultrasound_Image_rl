<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ultrasound Image Segmentation and Reinforcement Learning Navigation</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #0366d6;
        }
        a {
            color: #0366d6;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        pre {
            background-color: #f6f8fa;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 16px;
            overflow: auto;
            font-family: SFMono-Regular, Consolas, "Liberation Mono", Menlo, monospace;
            font-size: 85%;
        }
        code {
            background-color: rgba(27, 31, 35, 0.05);
            border-radius: 3px;
            font-family: SFMono-Regular, Consolas, "Liberation Mono", Menlo, monospace;
            font-size: 85%;
            padding: 0.2em 0.4em;
        }
        .container {
            background-color: #f6f8fa;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 20px;
            margin-bottom: 20px;
        }
        .back-link {
            margin-bottom: 20px;
            display: inline-block;
        }
        .caption {
            font-size: 0.9em;
            color: #666;
            text-align: center;
            margin-top: 5px;
            font-style: italic;
        }
        .image-container {
            margin: 20px 0;
            text-align: center;
        }
        .video-container {
            margin: 20px 0;
        }
        .limitations-box {
            background-color: #fff8dc;
            border: 1px solid #e6d9a3;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
        }
        .challenges-box {
            background-color: #f8f8ff;
            border: 1px solid #d9d9e6;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <a href="index.html" class="back-link">‚Üê Back to Home</a>
    
    <h1>Ultrasound Image Segmentation and Reinforcement Learning Navigation</h1>
    
    <h2>Introduction</h2>
    <p>
        In this project, I developed a system that combines deep learning-based image segmentation with reinforcement learning 
        to automatically navigate to regions of interest in ultrasound images. This approach has potential applications in 
        medical imaging and robotic ultrasound guidance.
    </p>
    
    <h2>Project Overview</h2>
    <p>The project consists of three main components:</p>
    <ol>
        <li><strong>Image Segmentation</strong>: A ResNet-based U-Net model trained to segment regions of interest in abdominal ultrasound images.</li>
        <li><strong>Center Detection</strong>: An algorithm to find the centers of the segmented regions.</li>
        <li><strong>Reinforcement Learning Navigation</strong>: A DQN (Deep Q-Network) agent trained to navigate to the centers of the segmented regions.</li>
    </ol>
    
    <h2>Image Segmentation with ResNet U-Net</h2>
    <p>
        For the segmentation task, I used a U-Net architecture with a ResNet18 backbone. This model was trained on a dataset 
        of abdominal ultrasound images to identify regions of interest.
    </p>
    
    <div class="image-container">
        <h3>Segmentation Results</h3>
        <img src="./images/segmentation_example.png" alt="Segmentation Example" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; padding: 5px; margin: 10px 0;">
        <p class="caption">Example of segmentation results on an abdominal ultrasound image. The red overlay shows the segmented region of interest.</p>
    </div>
    
    <!-- Code section omitted for brevity -->
    
    <h2>Finding Centers of Segmented Regions</h2>
    <p>After segmentation, I implemented an algorithm to find the centers of the segmented regions:</p>
    
    <!-- Code section omitted for brevity -->
    
    <h2>Reinforcement Learning Navigation</h2>
    <p>
        For the navigation task, I implemented a DQN agent with experience replay to learn how to navigate to the centers 
        of the segmented regions. The agent was trained to move a viewing window across the ultrasound image to find the 
        region of interest.
    </p>
    
    <h3>Environment</h3>
    <p>The environment simulates a moving ultrasound probe that can navigate across the image:</p>
    
    <!-- Code section omitted for brevity -->
    
    <h3>Addressing Oscillation Issues</h3>
    <p>
        One of the key challenges in this project was addressing the oscillating behavior of the RL agent. I implemented 
        several improvements to reduce oscillations:
    </p>
    
    <ol>
        <li><strong>Position History Tracking</strong>: Keeping track of recent positions to detect oscillations.</li>
        <li><strong>Oscillation Penalty</strong>: Adding a penalty to the reward function when oscillations are detected.</li>
        <li><strong>Momentum</strong>: Adding momentum to the agent's movements to make them smoother.</li>
        <li><strong>Improved Reward Function</strong>: Using a smoother reward function with a quadratic distance component.</li>
    </ol>
    
    <!-- Code section omitted for brevity -->
    
    <h2>Results</h2>
    <p>
        The trained agent successfully navigates to the centers of the segmented regions with a high success rate. 
        The improvements to address oscillations significantly enhanced the agent's performance.
    </p>
    
    <div class="image-container">
        <h3>Training Metrics</h3>
        <img src="./images/training_metrics.png" alt="Training Metrics" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; padding: 5px; margin: 10px 0;">
        <p class="caption">Training metrics showing the agent's performance improvement over time. The graphs show episode rewards, episode lengths, final distances to target, oscillation counts, training loss, and reward moving average.</p>
    </div>
    
    <h3>Example Navigation</h3>
    <p>Here's an example of the agent navigating to a segmented region:</p>
    <ol>
        <li>The agent starts at a random position in the image.</li>
        <li>It uses the segmentation model to identify the region of interest.</li>
        <li>It navigates to the center of the segmented region using the learned policy.</li>
        <li>The agent successfully reaches the target with minimal oscillations.</li>
    </ol>
    
    <div class="video-container" style="display: flex; justify-content: space-between; flex-wrap: wrap; margin: 20px 0;">
        <div style="flex: 1; min-width: 300px; margin: 10px;">
            <h4>Training Episode</h4>
            <img src="./images/episode_500.gif" alt="Training Episode" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; padding: 5px;">
            <p class="caption">Visualization of the agent's navigation during training (episode 500). The agent learns to navigate efficiently to the target center.</p>
        </div>
        <div style="flex: 1; min-width: 300px; margin: 10px;">
            <h4>Evaluation Episode</h4>
            <img src="./images/evaluation_episode.gif" alt="Evaluation Episode" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; padding: 5px;">
            <p class="caption">Visualization of the agent's navigation during evaluation. The agent successfully navigates to the target center with minimal oscillations.</p>
        </div>
    </div>
    
    <h2>Limitations and Challenges</h2>
    
    <div class="limitations-box">
        <h3>Image Quality Limitations</h3>
        <p>
            There is a significant gap between the image quality used in training and what would be encountered in real-world ultrasound probe motion. Several factors impact the performance of the model in real-world scenarios:
        </p>
        <ul>
            <li><strong>Lighting Conditions</strong>: Variations in lighting can significantly affect ultrasound image quality. The training data may not capture the full range of lighting conditions encountered in clinical settings.</li>
            <li><strong>Probe Contact</strong>: The quality of contact between the ultrasound probe and the skin surface varies in practice. Poor contact can lead to artifacts and reduced image clarity that the model hasn't been trained to handle.</li>
            <li><strong>Patient Variability</strong>: Anatomical differences between patients (tissue density, body composition, etc.) create variations in ultrasound images that may not be well-represented in the training data.</li>
            <li><strong>Motion Artifacts</strong>: Patient movement during scanning introduces motion artifacts that can confuse the segmentation model.</li>
        </ul>
        
        <h4>Example of Failure Cases</h4>
            <div class="video-container" style="display: flex; justify-content: space-between; flex-wrap: wrap; margin: 20px 0;">
        <div style="flex: 1; min-width: 300px; margin: 10px;">
            <h4>Training Episode</h4>
            <img src="./images/episode_500.gif" alt="Training Episode" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; padding: 5px;">
        </div>
        <div style="flex: 1; min-width: 300px; margin: 10px;">
            <h4>Evaluation Episode</h4>
            <img src="./images/evaluation_episode.gif" alt="Evaluation Episode" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; padding: 5px;">
        </div>
    </div>
        <p>
            When tested on new data with different image characteristics, the model sometimes fails to properly identify the target regions. These failure cases highlight the importance of diverse training data that captures the full range of conditions encountered in practice.
        </p>
        <!-- Add GIF from new_test data results folder showing failure cases -->
    </div>
    
    <div class="challenges-box">
        <h3>Training Data Sequence Importance</h3>
        <p>
            The sequence and quality of images used during training significantly impact the model's performance. Key observations include:
        </p>
        <ul>
            <li><strong>Data Consistency</strong>: Consistent image quality across the training dataset leads to better generalization.</li>
            <li><strong>Sequential Learning</strong>: The order in which examples are presented during training affects how well the agent learns navigation strategies.</li>
            <li><strong>Confidence vs. Distance Factors</strong>: In real-world deployment, we won't have prior knowledge of target locations. If the model relies too heavily on distance-based rewards rather than developing confidence in identifying anatomical features, it may struggle in practical applications.</li>
        </ul>
        
        <h4>Balancing Confidence and Distance</h4>
        <p>
            For robust real-world performance, the model needs to balance:
        </p>
        <ul>
            <li><strong>Feature Recognition</strong>: Learning to identify anatomical features regardless of their position in the image</li>
            <li><strong>Efficient Navigation</strong>: Developing strategies to move toward identified targets with minimal steps</li>
            <li><strong>Uncertainty Handling</strong>: Gracefully handling cases where target identification is uncertain</li>
        </ul>
    </div>
    
    <h2>Conclusion</h2>
    <p>
        This project demonstrates the potential of combining deep learning-based image segmentation with reinforcement 
        learning for automated navigation in medical imaging. The approach could be extended to real-world applications 
        such as robotic ultrasound guidance, where a robot could automatically position an ultrasound probe to capture 
        images of specific anatomical structures.
    </p>
    
    <p>Future work could include:</p>
    <ol>
        <li>Training on a larger and more diverse dataset to address image quality limitations</li>
        <li>Implementing a continuous action space for smoother navigation</li>
        <li>Testing on real-time ultrasound data with varying conditions</li>
        <li>Integrating with a robotic system for physical probe positioning</li>
        <li>Developing more robust feature recognition to reduce dependence on distance-based navigation</li>
        <li>Implementing adaptive techniques to handle varying image quality during operation</li>
    </ol>
    
    <div class="container">
        <h2>Code Repository and Feedback</h2>
        <p>
            The complete code for this project is available on 
            <a href="https://github.com/AnandMayank/Ultrasound_Image_rl" target="_blank">GitHub</a>.
            Feel free to explore the code, try it out, and adapt it for your own projects.
        </p>
        <p>
            Have questions, suggestions, or feedback? I'd love to hear from you! You can:
        </p>
        <ul>
            <li><a href="https://github.com/AnandMayank/Ultrasound_Image_rl/issues/new" target="_blank">Open an issue</a> on GitHub</li>
            <li>Connect with me on <a href="https://linkedin.com/in/anandmayank" target="_blank">LinkedIn</a></li>
            <li>Send me an <a href="mailto:your.email@example.com">email</a></li>
        </ul>
    </div>
    
    <footer>
        <p>&copy; 2024 Anand Mayank | All Rights Reserved</p>
    </footer>
</body>
</html>
